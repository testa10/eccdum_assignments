{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9dc109cd",
      "metadata": {
        "id": "9dc109cd"
      },
      "source": [
        "# Notebook overview\n",
        "\n",
        "The objective of this notebook is to offer a first approximation at Image Classification problems.\n",
        "\n",
        "For this, we will be using a very popular library `PyTorch` and a DataSet of fruits and vegetables.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "❗ **Before running the notebook in colab, go to the top menu: `Runtime` > `Change runtime type` > `Hardware accelerator` and select the option \"T4 GPU\" to ensure faster execution.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48b69de3",
      "metadata": {
        "id": "48b69de3"
      },
      "outputs": [],
      "source": [
        "!pip install -q eccd_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f70d1fb1",
      "metadata": {
        "id": "f70d1fb1"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "import torchvision.transforms.v2 as transforms\n",
        "\n",
        "from eccd_datasets import load_images\n",
        "from PIL import Image\n",
        "from torchvision import models\n",
        "\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aff92624",
      "metadata": {
        "id": "aff92624"
      },
      "outputs": [],
      "source": [
        "# Download the original lables used when training a resnet\n",
        "\n",
        "resnet_labels = {\n",
        "    int(index): label\n",
        "    for index, (id, label) in requests.get(\n",
        "        \"https://files.fast.ai/models/imagenet_class_index.json\"\n",
        "    )\n",
        "    .json()\n",
        "    .items()\n",
        "}\n",
        "resnet_labels[11]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5933cb11",
      "metadata": {
        "id": "5933cb11"
      },
      "source": [
        "# Exploring the dataset\n",
        "\n",
        "First, we invite you to go to the dataset folder and explore the content and structure of the project.\n",
        "\n",
        "The dataset used in this notebook consists on a subset from the dataset located [here](https://github.com/marcusklasson/GroceryStoreDataset)\n",
        "\n",
        "Once that is done, we can start looking at what is included in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79a6d06",
      "metadata": {
        "id": "e79a6d06"
      },
      "outputs": [],
      "source": [
        "df_images = load_images()\n",
        "df_images.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1999914",
      "metadata": {
        "id": "a1999914"
      },
      "source": [
        "### Looking at the images\n",
        "\n",
        "We can use the `PIL` library to look at the images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e76d229d",
      "metadata": {
        "id": "e76d229d"
      },
      "outputs": [],
      "source": [
        "def load_image_data(image_data):\n",
        "    return Image.open(io.BytesIO(image_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28bbde24",
      "metadata": {
        "id": "28bbde24"
      },
      "outputs": [],
      "source": [
        "image = load_image_data(df_images.iloc[0][\"image_data\"])\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5ee351e",
      "metadata": {
        "id": "e5ee351e"
      },
      "source": [
        "### Images as matrices\n",
        "\n",
        "We can also look look at the matrix representation of each image using numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457c4a6e",
      "metadata": {
        "id": "457c4a6e"
      },
      "outputs": [],
      "source": [
        "I = np.array(image)\n",
        "print(\"Image shape\", I.shape)\n",
        "print(f\"Image range in each coordinate: [{I.min()}, {I.max()}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "820b7cbd",
      "metadata": {
        "id": "820b7cbd"
      },
      "source": [
        "And we can modify the image manually by changing the values of the matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5efae5a",
      "metadata": {
        "id": "d5efae5a"
      },
      "outputs": [],
      "source": [
        "new_I = I.copy()\n",
        "new_I[:, :, 0] = 0 # Killing the red channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74c81bdc",
      "metadata": {
        "id": "74c81bdc"
      },
      "outputs": [],
      "source": [
        "plt.imshow(new_I.astype(int))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "154440c3",
      "metadata": {
        "id": "154440c3"
      },
      "source": [
        "# PyTorch Transformations\n",
        "\n",
        "The same way we normalize tabular data with Standard and MinMax scalers, we need to normalize image data.\n",
        "\n",
        "We will proceed to explore some of the most used transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678c3a12",
      "metadata": {
        "id": "678c3a12"
      },
      "source": [
        "## Resizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673b32ea",
      "metadata": {
        "id": "673b32ea"
      },
      "outputs": [],
      "source": [
        "resize_image = transforms.Resize((100, 100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305d958b",
      "metadata": {
        "id": "305d958b"
      },
      "outputs": [],
      "source": [
        "resized_image = resize_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00bb192e",
      "metadata": {
        "id": "00bb192e"
      },
      "outputs": [],
      "source": [
        "plt.imshow(resized_image)\n",
        "plt.title(f\"New shape: {np.array(resized_image).shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7987122f",
      "metadata": {
        "id": "7987122f"
      },
      "source": [
        "## Center Crop\n",
        "\n",
        "Implement a transformation for croping and centering (hint: there is a transformation that does that)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba55737e",
      "metadata": {
        "id": "ba55737e"
      },
      "outputs": [],
      "source": [
        "type(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a53719a8",
      "metadata": {
        "id": "a53719a8"
      },
      "outputs": [],
      "source": [
        "def center_crop_transformation(image, size: int) -> np.array:\n",
        "    \"\"\"\n",
        "    This function uses a pytorch transformation to\n",
        "    center and crop the image\n",
        "    \"\"\"\n",
        "    # Write your code here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eda552a",
      "metadata": {
        "id": "0eda552a"
      },
      "outputs": [],
      "source": [
        "answer_center_crop = center_crop_transformation(image, 150)\n",
        "plt.imshow(answer_center_crop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1997931c",
      "metadata": {
        "id": "1997931c"
      },
      "outputs": [],
      "source": [
        "assert np.array(answer_center_crop).shape == (150, 150, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "305aa942",
      "metadata": {
        "id": "305aa942"
      },
      "source": [
        "## RandomResizedCrop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f539a597",
      "metadata": {
        "id": "f539a597"
      },
      "outputs": [],
      "source": [
        "print(\"Image original size: \", np.array(image).shape)\n",
        "fig, ax = plt.subplots(1, 6, figsize=(20, 4))\n",
        "for i, size in enumerate([50, 100, 150, 200, 300, 500]):\n",
        "\n",
        "    transformation = transforms.RandomResizedCrop(size)\n",
        "\n",
        "    crp_img = transformation(image)\n",
        "    ax[i].imshow(crp_img)\n",
        "    ax[i].set_title(np.array(crp_img).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa0b4094",
      "metadata": {
        "id": "fa0b4094"
      },
      "source": [
        "## Random Horizontal Flip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b203cc",
      "metadata": {
        "id": "32b203cc"
      },
      "outputs": [],
      "source": [
        "transformation = transforms.RandomHorizontalFlip()\n",
        "\n",
        "maybe_flipped = [transformation(image) for _ in range(5)]\n",
        "\n",
        "plt.imshow(np.hstack([np.array(img) for img in maybe_flipped]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad8ade12",
      "metadata": {
        "id": "ad8ade12"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "The same way we normalize columns for tabular data, here we normalize each image according to the mean and standard deviation of each colour channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6ecba99",
      "metadata": {
        "id": "a6ecba99"
      },
      "outputs": [],
      "source": [
        "two_images = [\n",
        "    load_image_data(row[\"image_data\"]) for _, row in df_images.iloc[:2].iterrows()\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32b606dc",
      "metadata": {
        "id": "32b606dc"
      },
      "outputs": [],
      "source": [
        "two_image_dataset = np.array([np.array(img) for img in two_images])\n",
        "two_image_dataset.shape\n",
        "\n",
        "plt.imshow(np.hstack([np.array(img) for img in two_images]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "586c4d9d",
      "metadata": {
        "id": "586c4d9d"
      },
      "outputs": [],
      "source": [
        "two_image_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d5a9a68",
      "metadata": {
        "id": "4d5a9a68"
      },
      "outputs": [],
      "source": [
        "np.mean(two_image_dataset, axis=(0, 1, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfd2472f",
      "metadata": {
        "id": "dfd2472f"
      },
      "outputs": [],
      "source": [
        "transformation = transforms.Compose(\n",
        "    [\n",
        "        transforms.ToImage(),\n",
        "        transforms.ToDtype(torch.float32, scale=True),\n",
        "        transforms.Normalize(\n",
        "            np.mean(two_image_dataset, axis=(0, 1, 2)),\n",
        "            np.std(two_image_dataset, axis=(0, 1, 2)),\n",
        "        ),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a812a85e",
      "metadata": {
        "id": "a812a85e"
      },
      "outputs": [],
      "source": [
        "transformed_two_image_dataset = [transformation(img) for img in two_image_dataset]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0036724",
      "metadata": {
        "id": "a0036724"
      },
      "outputs": [],
      "source": [
        "transformed_two_image_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd8f050a",
      "metadata": {
        "id": "dd8f050a"
      },
      "source": [
        "# Using ImageNet\n",
        "\n",
        "Since training a large neural network requires lots of data and computing power, often we download a pre-trained neural network, which we can later fine-tune.\n",
        "\n",
        "Here, we will download an ImageNet network.\n",
        "\n",
        "Remember that since the network is already trained with a specific dataset, when evaluating new images, we need transform them using the same transformations used for training. In particular, that includes using the same normalizations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1250b760",
      "metadata": {
        "id": "1250b760"
      },
      "outputs": [],
      "source": [
        "resnet = models.resnet18(pretrained=True)\n",
        "# resnet = models.resnet34(pretrained=True)\n",
        "# resnet = models.resnet50(pretrained=True)\n",
        "# resnet = models.resnet101(pretrained=True)\n",
        "# resnet = models.resnet152(pretrained=True)\n",
        "resnet.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36838c5c",
      "metadata": {
        "id": "36838c5c"
      },
      "outputs": [],
      "source": [
        "resnet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c67108b",
      "metadata": {
        "id": "4c67108b"
      },
      "source": [
        "We load a maping from resnet integer labels to the actual categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7ca7880",
      "metadata": {
        "id": "a7ca7880"
      },
      "outputs": [],
      "source": [
        "def predict_using_resnet(image):\n",
        "    \"\"\"\n",
        "    This image uses the resnet as is to\n",
        "    predict an image.\n",
        "    Remember to apply the correct transformations\n",
        "    to the image before feeding it to the network.\n",
        "\n",
        "    The following link might be useful: https://pytorch.org/hub/pytorch_vision_resnet/\n",
        "    \"\"\"\n",
        "\n",
        "    # Write your code here\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8896d58b",
      "metadata": {
        "id": "8896d58b"
      },
      "outputs": [],
      "source": [
        "img1 = (\n",
        "    load_image_data(\n",
        "        df_images[\n",
        "            df_images[\"coarse_cat\"] == \"Apple\"\n",
        "        ]\n",
        "        .iloc[0]\n",
        "        [\"image_data\"]\n",
        "    )\n",
        ")\n",
        "img1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6349f374",
      "metadata": {
        "id": "6349f374"
      },
      "outputs": [],
      "source": [
        "img2 = (\n",
        "    load_image_data(\n",
        "        df_images[\n",
        "            df_images[\"coarse_cat\"] == \"Orange\"\n",
        "        ]\n",
        "        .iloc[0]\n",
        "        [\"image_data\"]\n",
        "    )\n",
        ")\n",
        "img2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6ffea2",
      "metadata": {
        "id": "7a6ffea2"
      },
      "outputs": [],
      "source": [
        "img3 = (\n",
        "    load_image_data(\n",
        "        df_images[\n",
        "            df_images[\"coarse_cat\"] == \"Pear\"\n",
        "        ]\n",
        "        .iloc[0]\n",
        "        [\"image_data\"]\n",
        "    )\n",
        ")\n",
        "img3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21b50d89",
      "metadata": {
        "id": "21b50d89"
      },
      "outputs": [],
      "source": [
        "pred1 = predict_using_resnet(img1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd173ece",
      "metadata": {
        "id": "cd173ece"
      },
      "outputs": [],
      "source": [
        "assert pred1 == \"lemon\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f49449e",
      "metadata": {
        "id": "4f49449e"
      },
      "outputs": [],
      "source": [
        "pred2 = predict_using_resnet(img2)\n",
        "print(pred2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49ddc7de",
      "metadata": {
        "id": "49ddc7de"
      },
      "outputs": [],
      "source": [
        "pred3 = predict_using_resnet(img3)\n",
        "print(pred3)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}